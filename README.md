#  Text-to-Action LLM

A research-grade system that converts natural language instructions into structured JSON action plans for scene manipulation, with support for **chained commands** and **continuous motion animation**.

##  Project Overview

This project demonstrates end-to-end LLM fine-tuning and deployment for instruction-to-action parsing. Given a natural language command, the system outputs a structured JSON response:

**Single Action:**
```json
{
  "object": "red box",
  "action": "move",
  "target_position": "blue platform"
}
```

**Chained Commands:**
```json
{
  "sequence": [
    {"object": "red box", "action": "move", "target_position": "top shelf"},
    {"object": "green sphere", "action": "rotate", "target_position": "90 degrees"}
  ]
}
```

##  Features

- ** LLM Reasoning**: Model infers objects, actions, and positions - no hardcoded rules
- **ğŸ”— Chained Commands**: Support for sequential multi-step actions ("then", "after")
- ** Smooth Animation**: Continuous motion visualization with easing
- ** Dynamic Objects**: Create/remove objects at runtime
- ** Modern UI**: Clean, responsive interface with execution logs

##  System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Frontend     â”‚â”€â”€â”€â”€â–¶â”‚    Backend      â”‚â”€â”€â”€â”€â–¶â”‚   Fine-tuned    â”‚
â”‚  (Three.js/JS)  â”‚     â”‚   (FastAPI)     â”‚     â”‚   LLaMA + LoRA  â”‚
â”‚                 â”‚â—€â”€â”€â”€â”€â”‚                 â”‚â—€â”€â”€â”€â”€â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Visualize              /infer API            Structured JSON
```

See [docs/architecture.md](docs/architecture.md) for detailed system design.

##  Repository Structure

```
text-to-action-llm/
â”œâ”€â”€ research/          # ML experiments, notebooks, datasets
â”‚   â”œâ”€â”€ colab/         # Training & inference notebooks
â”‚   â”œâ”€â”€ data/          # Instruction-action datasets
â”‚   â””â”€â”€ experiments/   # Ablation studies & notes
â”œâ”€â”€ backend/           # FastAPI inference server
â”‚   â””â”€â”€ app/           # API, LLM modules, utilities
â”œâ”€â”€ frontend/          # 3D visualization (vanilla JS)
â””â”€â”€ deployment/        # Setup guides for Colab, ngrok, Ollama
```

##  Quick Start

### 1. Backend (FastAPI)

```bash
cd backend
pip install -r requirements.txt
uvicorn app.main:app --reload
```

### 2. Frontend

Open `frontend/index.html` in a browser, or serve with:

```bash
cd frontend
python -m http.server 8080
```

### 3. Training (Colab)

Open `research/colab/finetune_llama31_unsloth.ipynb` in Google Colab with GPU runtime.

## ğŸ”¬ Research Highlights

- **Model**: LLaMA 3.1 8B with QLoRA adapters
- **Training**: Unsloth for 2x faster fine-tuning
- **Dataset**: Synthetic instruction-action pairs
- **Output**: Pydantic-validated structured JSON

## ğŸ“Š Example Use Cases

| Instruction | Output Action |
|-------------|---------------|
| "Move the red box to the platform" | `{"object": "red box", "action": "move", ...}` |
| "Rotate the blue sphere 90 degrees" | `{"object": "blue sphere", "action": "rotate", ...}` |
| "Scale the green cube by 2x" | `{"object": "green cube", "action": "scale", ...}` |
| "Move red box to shelf, then rotate blue sphere" | `{"sequence": [{...}, {...}]}` |

## âœ… Assignment Requirements

| Requirement | Implementation |
|-------------|----------------|
| LLM runs locally | âœ… Ollama + fine-tuned LLaMA |
| Model reasoning (no hardcoding) | âœ… LLM infers all actions |
| Structured JSON output | âœ… Pydantic-validated |
| Motion visualization | âœ… Canvas animation |
| **Optional: Multiple objects** | âœ… Sequence-based |
| **Optional: Chained commands** | âœ… Async/await execution |
| **Optional: Continuous animation** | âœ… requestAnimationFrame |

## ğŸ› ï¸ Future Extensions

- [ ] Animation keyframe generation
- [ ] Real-time voice input
- [ ] Multi-modal scene understanding

##  License

MIT License - see [LICENSE](LICENSE) for details.

##  Contributing

Contributions welcome! Please read the architecture docs before submitting PRs.

---

*Built with â¤ï¸ for LLM research and practical deployment*
